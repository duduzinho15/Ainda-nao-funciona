"""
MÃ³dulo para busca de ofertas no site Promobit.

Este mÃ³dulo implementa um scraper para buscar ofertas de produtos de informÃ¡tica
no site Promobit, com tratamento de erros e anti-bloqueio.
"""

import asyncio
import logging
import random
import re
import time
from datetime import datetime
from typing import List, Dict, Optional, Any, Tuple, Callable, TypeVar, Type, Union
from functools import wraps

import aiohttp
import aiohttp.client_exceptions
from bs4 import BeautifulSoup

# Type variable for generic function return type
T = TypeVar("T")


def async_retry(
    max_retries: int = 3,
    initial_delay: float = 1.0,
    max_delay: float = 30.0,
    backoff_factor: float = 2.0,
    exceptions: Union[Type[Exception], Tuple[Type[Exception], ...]] = Exception,
) -> Callable:
    """
    A decorator for retrying async functions with exponential backoff.

    Args:
        max_retries: Maximum number of retry attempts
        initial_delay: Initial delay between retries in seconds
        max_delay: Maximum delay between retries in seconds
        backoff_factor: Factor by which the delay should increase on each retry
        exceptions: Exception(s) to catch and retry on
    """

    def decorator(func: Callable[..., T]) -> Callable[..., T]:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> T:
            delay = initial_delay
            last_exception = None

            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt == max_retries - 1:
                        raise

                    # Calculate next delay with exponential backoff and jitter
                    sleep_time = min(delay * (backoff_factor**attempt), max_delay)
                    jitter = random.uniform(0.5, 1.5)  # Add some randomness
                    actual_delay = sleep_time * jitter

                    logger.warning(
                        f"Attempt {attempt + 1} failed with error: {str(e)}. "
                        f"Retrying in {actual_delay:.2f} seconds..."
                    )

                    await asyncio.sleep(actual_delay)

            # This should never be reached due to the raise in the except block
            raise last_exception  # type: ignore

        return wrapper

    return decorator


# ConfiguraÃ§Ã£o de logging
logger = logging.getLogger("promobit_scraper")
logger.setLevel(logging.INFO)

# Cria handlers
console_handler = logging.StreamHandler()
file_handler = logging.FileHandler("promobit_scraper.log", encoding="utf-8")

# Define formato
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
console_handler.setFormatter(formatter)
file_handler.setFormatter(formatter)

# Adiciona handlers ao logger
if not logger.handlers:
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

# ConfiguraÃ§Ã£o de logging para aiohttp
aiohttp_logger = logging.getLogger("aiohttp")
aiohttp_logger.setLevel(logging.WARNING)  # Reduz o log do aiohttp para WARNING

# Tenta importar o mÃ³dulo Brotli para suporte a compactaÃ§Ã£o
try:
    import brotli

    BROTLI_AVAILABLE = True
except ImportError:
    logger.warning(
        "MÃ³dulo 'brotli' nÃ£o encontrado. O suporte a respostas Brotli estarÃ¡ desabilitado."
    )
    BROTLI_AVAILABLE = False

# ConfiguraÃ§Ãµes do scraper
BASE_URL = "https://www.promobit.com.br"
CATEGORIAS = {
    "informatica": "/promocoes/informatica/",
    "smartphones": "/promocoes/smartphones-tablets-e-telefones/",
    "eletronicos": "/promocoes/eletronicos-audio-e-video/",
    "games": "/promocoes/games/",
    "livros": "/promocoes/livros-ebooks-e-ereaders/",
    "moda": "/promocoes/moda-e-calcados-femininos/",
    "casa": "/promocoes/moveis-e-decoracao/",
    "esporte": "/promocoes/esporte-e-lazer/",
    "outros": "/promocoes/outros/",
}

# Headers para simular um navegador real
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br" if BROTLI_AVAILABLE else "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Cache-Control": "max-age=0",
}

# ConfiguraÃ§Ãµes de delay para evitar bloqueio
MIN_DELAY = 1.0
MAX_DELAY = 3.0


async def create_session() -> aiohttp.ClientSession:
    """
    Cria uma sessÃ£o aiohttp configurada para o scraper.

    Returns:
        aiohttp.ClientSession: SessÃ£o configurada
    """
    # ConfiguraÃ§Ãµes da sessÃ£o
    timeout = aiohttp.ClientTimeout(total=30, connect=10)
    connector = aiohttp.TCPConnector(
        limit=10, limit_per_host=5, ttl_dns_cache=300, use_dns_cache=True
    )

    # Cria a sessÃ£o
    session = aiohttp.ClientSession(
        headers=DEFAULT_HEADERS, timeout=timeout, connector=connector
    )

    return session


@async_retry(max_retries=3, initial_delay=1.0, max_delay=10.0)
async def fetch_page(
    session: aiohttp.ClientSession, url: str, timeout: int = 30
) -> Optional[str]:
    """
    Faz o download de uma pÃ¡gina do Promobit.

    Args:
        session: SessÃ£o aiohttp
        url: URL da pÃ¡gina a ser baixada
        timeout: Timeout em segundos

    Returns:
        str: ConteÃºdo HTML da pÃ¡gina ou None se falhar
    """
    try:
        logger.debug(f"Fazendo requisiÃ§Ã£o para: {url}")

        async with session.get(url, timeout=timeout) as response:
            response.raise_for_status()

            # Verifica o tipo de conteÃºdo
            content_type = response.headers.get("content-type", "").lower()
            if "text/html" not in content_type:
                logger.warning(f"Tipo de conteÃºdo inesperado: {content_type}")
                return None

            # LÃª o conteÃºdo
            content = await response.text()

            # Aguarda um pouco para nÃ£o sobrecarregar o servidor
            delay = random.uniform(MIN_DELAY, MAX_DELAY)
            await asyncio.sleep(delay)

            logger.debug(f"PÃ¡gina baixada com sucesso: {len(content)} caracteres")
            return content

    except aiohttp.ClientResponseError as e:
        logger.error(f"Erro HTTP {e.status} ao acessar {url}: {e}")
        return None
    except aiohttp.ClientError as e:
        logger.error(f"Erro de cliente ao acessar {url}: {e}")
        return None
    except asyncio.TimeoutError:
        logger.error(f"Timeout ao acessar {url}")
        return None
    except Exception as e:
        logger.error(f"Erro inesperado ao acessar {url}: {e}")
        return None


def extract_ofertas_from_html(html_content: str) -> List[Dict[str, Any]]:
    """
    Extrai ofertas do HTML de uma pÃ¡gina do Promobit.

    Args:
        html_content: ConteÃºdo HTML da pÃ¡gina

    Returns:
        List[Dict[str, Any]]: Lista de ofertas extraÃ­das
    """
    ofertas = []

    try:
        soup = BeautifulSoup(html_content, "html.parser")

        # Procura por links de oferta
        links_oferta = soup.find_all("a", href=re.compile(r"/oferta/"))

        logger.info(f"Encontrados {len(links_oferta)} links de oferta na pÃ¡gina")

        for link in links_oferta:
            try:
                oferta = extract_single_oferta(link)
                if oferta:
                    ofertas.append(oferta)
            except Exception as e:
                logger.error(f"Erro ao extrair oferta: {e}")
                continue

        logger.info(f"ExtraÃ­das {len(ofertas)} ofertas vÃ¡lidas")

    except Exception as e:
        logger.error(f"Erro ao processar HTML: {e}")

    return ofertas


def extract_single_oferta(link_element) -> Optional[Dict[str, Any]]:
    """
    Extrai informaÃ§Ãµes de uma oferta individual.

    Args:
        link_element: Elemento HTML do link da oferta

    Returns:
        Dict[str, Any]: DicionÃ¡rio com as informaÃ§Ãµes da oferta ou None se falhar
    """
    try:
        # Extrai o link da oferta
        href = link_element.get("href", "")
        if not href:
            return None

        # ConstrÃ³i a URL completa
        if href.startswith("/"):
            url_oferta = f"{BASE_URL}{href}"
        else:
            url_oferta = href

        # Extrai o ID da oferta da URL
        id_match = re.search(r"/oferta/([^/]+)/?$", href)
        if id_match:
            id_oferta = id_match.group(1)
        else:
            id_oferta = str(hash(href))[-8:]

        # Extrai o tÃ­tulo
        titulo = ""
        titulo_elem = link_element.find("span", class_=re.compile(r"line-clamp-2"))
        if titulo_elem:
            titulo = titulo_elem.get_text(strip=True)

        if not titulo:
            return None

        # Extrai o preÃ§o atual
        preco = ""
        preco_elem = link_element.find("span", class_=re.compile(r"text-primary-400"))
        if preco_elem:
            preco = preco_elem.get_text(strip=True)

        # Extrai o preÃ§o original (riscado)
        preco_original = ""
        preco_original_elem = link_element.find(
            "span", class_=re.compile(r"line-through")
        )
        if preco_original_elem:
            preco_original = preco_original_elem.get_text(strip=True)

        # Extrai a loja
        loja = ""
        loja_elem = link_element.find("span", class_=re.compile(r"font-bold.*text-sm"))
        if loja_elem:
            loja = loja_elem.get_text(strip=True)

        # Extrai a imagem
        url_imagem = ""
        img_elem = link_element.find("img")
        if img_elem:
            img_src = img_elem.get("src", "")
            if img_src:
                if img_src.startswith("//"):
                    url_imagem = f"https:{img_src}"
                elif img_src.startswith("/"):
                    url_imagem = f"{BASE_URL}{img_src}"
                else:
                    url_imagem = img_src

        # Extrai tags/badges
        tags = []
        badges = link_element.find_all("div", class_=re.compile(r"bg-neutral-high-300"))
        if badges:
            tags = [badge.get_text(strip=True) for badge in badges]

        # Calcula o desconto se houver preÃ§o original
        desconto = 0
        if preco_original and preco:
            try:
                # Remove caracteres nÃ£o numÃ©ricos e converte para float
                preco_orig = float(
                    re.sub(r"[^\d.,]", "", preco_original).replace(",", ".")
                )
                preco_atual = float(re.sub(r"[^\d.,]", "", preco).replace(",", "."))

                if preco_orig > 0:
                    desconto = ((preco_orig - preco_atual) / preco_orig) * 100
            except (ValueError, ZeroDivisionError):
                pass

        # Cria o dicionÃ¡rio da oferta
        oferta = {
            "id_produto": id_oferta,
            "loja": loja,
            "titulo": titulo,
            "preco": preco,
            "url_produto": url_oferta,
            "url_imagem": url_imagem,
            "preco_original": preco_original,
            "desconto": round(desconto, 2),
            "tags": tags,
            "data_extraÃ§Ã£o": datetime.now().isoformat(),
        }

        return oferta

    except Exception as e:
        logger.error(f"Erro ao extrair oferta individual: {e}")
        return None


async def processar_pagina(
    session: aiohttp.ClientSession,
    url: str,
    min_desconto: int = 10,
    min_preco: Optional[float] = None,
    max_preco: Optional[float] = None,
    min_avaliacao: float = 4.0,
    min_votos: int = 10,
    apenas_frete_gratis: bool = True,
    apenas_lojas_oficiais: bool = True,
) -> List[Dict[str, Any]]:
    """
    Processa uma pÃ¡gina do Promobit e extrai as ofertas.

    Args:
        session: SessÃ£o aiohttp
        url: URL da pÃ¡gina a ser processada
        min_desconto: Percentual mÃ­nimo de desconto
        min_preco: PreÃ§o mÃ­nimo
        max_preco: PreÃ§o mÃ¡ximo
        min_avaliacao: AvaliaÃ§Ã£o mÃ­nima
        min_votos: NÃºmero mÃ­nimo de votos
        apenas_frete_gratis: Se True, retorna apenas ofertas com frete grÃ¡tis
        apenas_lojas_oficiais: Se True, retorna apenas ofertas de lojas oficiais

    Returns:
        List[Dict[str, Any]]: Lista de ofertas encontradas na pÃ¡gina
    """
    try:
        logger.info(f"Processando pÃ¡gina: {url}")

        # Faz o download da pÃ¡gina
        html_content = await fetch_page(session, url)
        if not html_content:
            logger.warning(f"Falha ao baixar pÃ¡gina: {url}")
            return []

        # Extrai as ofertas do HTML
        ofertas = extract_ofertas_from_html(html_content)

        # Filtra as ofertas baseado nos critÃ©rios
        ofertas_filtradas = filter_ofertas(
            ofertas,
            min_desconto=min_desconto,
            min_preco=min_preco,
            max_preco=max_preco,
            apenas_frete_gratis=apenas_frete_gratis,
            apenas_lojas_oficiais=apenas_lojas_oficiais,
        )

        logger.info(
            f"PÃ¡gina processada: {len(ofertas)} ofertas encontradas, {len(ofertas_filtradas)} apÃ³s filtros"
        )

        return ofertas_filtradas

    except Exception as e:
        logger.error(f"Erro ao processar pÃ¡gina {url}: {e}")
        return []


def filter_ofertas(
    ofertas: List[Dict[str, Any]],
    min_desconto: int = 10,
    min_preco: Optional[float] = None,
    max_preco: Optional[float] = None,
    apenas_frete_gratis: bool = True,
    apenas_lojas_oficiais: bool = True,
) -> List[Dict[str, Any]]:
    """
    Filtra as ofertas baseado nos critÃ©rios especificados.

    Args:
        ofertas: Lista de ofertas a serem filtradas
        min_desconto: Percentual mÃ­nimo de desconto
        min_preco: PreÃ§o mÃ­nimo
        max_preco: PreÃ§o mÃ¡ximo
        apenas_frete_gratis: Se True, retorna apenas ofertas com frete grÃ¡tis
        apenas_lojas_oficiais: Se True, retorna apenas ofertas de lojas oficiais

    Returns:
        List[Dict[str, Any]]: Lista de ofertas filtradas
    """
    ofertas_filtradas = []

    for oferta in ofertas:
        try:
            # Filtra por desconto mÃ­nimo
            if oferta.get("desconto", 0) < min_desconto:
                continue

            # Filtra por preÃ§o mÃ­nimo
            if min_preco is not None:
                preco_atual = extract_price_value(oferta.get("preco", ""))
                if preco_atual is None or preco_atual < min_preco:
                    continue

            # Filtra por preÃ§o mÃ¡ximo
            if max_preco is not None:
                preco_atual = extract_price_value(oferta.get("preco", ""))
                if preco_atual is None or preco_atual > max_preco:
                    continue

            # Filtra por frete grÃ¡tis
            if apenas_frete_gratis:
                tags = oferta.get("tags", [])
                if not any(
                    "frete" in tag.lower() and "grÃ¡tis" in tag.lower() for tag in tags
                ):
                    continue

            # Filtra por lojas oficiais
            if apenas_lojas_oficiais:
                loja = oferta.get("loja", "").lower()
                lojas_oficiais = [
                    "amazon",
                    "magazine luiza",
                    "kabum",
                    "americanas",
                    "casas bahia",
                ]
                if not any(loja_oficial in loja for loja_oficial in lojas_oficiais):
                    continue

            ofertas_filtradas.append(oferta)

        except Exception as e:
            logger.error(f"Erro ao filtrar oferta: {e}")
            continue

    return ofertas_filtradas


def extract_price_value(price_str: str) -> Optional[float]:
    """
    Extrai o valor numÃ©rico de uma string de preÃ§o.

    Args:
        price_str: String contendo o preÃ§o

    Returns:
        float: Valor numÃ©rico do preÃ§o ou None se falhar
    """
    try:
        if not price_str:
            return None

        # Remove caracteres nÃ£o numÃ©ricos e converte para float
        clean_price = re.sub(r"[^\d.,]", "", price_str)
        clean_price = clean_price.replace(",", ".")

        if clean_price:
            return float(clean_price)

        return None

    except (ValueError, TypeError):
        return None


async def buscar_ofertas_promobit(
    session: Optional[aiohttp.ClientSession] = None,
    max_paginas: int = 3,
    min_desconto: int = 10,
    categoria: str = "informatica",
    min_preco: Optional[float] = None,
    max_preco: Optional[float] = None,
    min_avaliacao: float = 4.0,
    min_votos: int = 10,
    apenas_frete_gratis: bool = True,
    apenas_lojas_oficiais: bool = True,
    max_requests: int = 10,
    request_timeout: int = 30,
) -> List[Dict[str, Any]]:
    """
    Busca ofertas no Promobit com base nos critÃ©rios fornecidos.

    Args:
        session: SessÃ£o aiohttp (opcional). Se nÃ£o for fornecida, uma nova serÃ¡ criada.
        max_paginas: NÃºmero mÃ¡ximo de pÃ¡ginas para buscar (padrÃ£o: 3)
        min_desconto: Percentual mÃ­nimo de desconto para considerar (padrÃ£o: 10%)
        categoria: Categoria de produtos para buscar (padrÃ£o: 'informatica')
        min_preco: PreÃ§o mÃ­nimo para filtrar ofertas (opcional)
        max_preco: PreÃ§o mÃ¡ximo para filtrar ofertas (opcional)
        min_avaliacao: AvaliaÃ§Ã£o mÃ­nima do produto (0-5, padrÃ£o: 4.0)
        min_votos: NÃºmero mÃ­nimo de votos para considerar avaliaÃ§Ã£o (padrÃ£o: 10)
        apenas_frete_gratis: Se True, retorna apenas ofertas com frete grÃ¡tis (padrÃ£o: True)
        apenas_lojas_oficiais: Se True, retorna apenas ofertas de lojas oficiais (padrÃ£o: True)
        max_requests: NÃºmero mÃ¡ximo de requisiÃ§Ãµes simultÃ¢neas (padrÃ£o: 10)
        request_timeout: Timeout em segundos para cada requisiÃ§Ã£o (padrÃ£o: 30)

    Returns:
        Lista de dicionÃ¡rios contendo as ofertas encontradas
    """
    # Cria uma nova sessÃ£o se nÃ£o for fornecida
    close_session = False
    if session is None:
        session = await create_session()
        close_session = True

    try:
        # ObtÃ©m a URL base para a categoria
        categoria_url = CATEGORIAS.get(categoria.lower(), CATEGORIAS["informatica"])
        base_url = f"{BASE_URL}{categoria_url}"

        logger.info(
            f"Iniciando busca por ofertas na categoria: {categoria} (URL: {base_url})"
        )

        # Lista para armazenar todas as ofertas encontradas
        todas_as_ofertas = []

        # Lista de tarefas para buscar pÃ¡ginas em paralelo
        tasks = []

        # Cria tarefas para buscar cada pÃ¡gina
        for page_num in range(1, max_paginas + 1):
            # ConstrÃ³i a URL da pÃ¡gina
            if page_num == 1:
                url = base_url
            else:
                url = f"{base_url}?page={page_num}"

            # Adiciona a tarefa Ã  lista
            tasks.append(
                processar_pagina(
                    session=session,
                    url=url,
                    min_desconto=min_desconto,
                    min_preco=min_preco,
                    max_preco=max_preco,
                    min_avaliacao=min_avaliacao,
                    min_votos=min_votos,
                    apenas_frete_gratis=apenas_frete_gratis,
                    apenas_lojas_oficiais=apenas_lojas_oficiais,
                )
            )

        # Executa as tarefas em paralelo, limitando o nÃºmero de requisiÃ§Ãµes simultÃ¢neas
        sem = asyncio.Semaphore(max_requests)

        async def bounded_gather(tasks):
            async def sem_task(task):
                async with sem:
                    return await task

            return await asyncio.gather(*(sem_task(task) for task in tasks))

        # Aguarda a conclusÃ£o de todas as tarefas
        resultados = await bounded_gather(tasks)

        # Processa os resultados
        for resultado in resultados:
            if isinstance(resultado, list):
                todas_as_ofertas.extend(resultado)
            elif isinstance(resultado, Exception):
                logger.error(f"Erro ao processar pÃ¡gina: {resultado}", exc_info=True)

        # Remove ofertas duplicadas (mesma URL)
        ofertas_unicas = {}
        for oferta in todas_as_ofertas:
            url = oferta.get("url_produto", "")
            if url and url not in ofertas_unicas:
                ofertas_unicas[url] = oferta

        ofertas_finais = list(ofertas_unicas.values())

        # Ordena por desconto (maior primeiro)
        ofertas_finais.sort(key=lambda x: x.get("desconto", 0), reverse=True)

        logger.info(
            f"Busca concluÃ­da: {len(ofertas_finais)} ofertas Ãºnicas encontradas"
        )

        return ofertas_finais

    except Exception as e:
        logger.error(f"Erro durante a busca de ofertas: {e}", exc_info=True)
        return []

    finally:
        # Fecha a sessÃ£o se foi criada por esta funÃ§Ã£o
        if close_session and session:
            await session.close()


# FunÃ§Ã£o sÃ­ncrona para compatibilidade
def buscar_ofertas_promobit_sync(
    max_paginas: int = 3,
    min_desconto: int = 0,  # Reduzido de 10 para 0
    categoria: str = "informatica",
    min_preco: Optional[float] = None,
    max_preco: Optional[float] = None,
    apenas_frete_gratis: bool = False,  # Mudado de True para False
    apenas_lojas_oficiais: bool = False,  # Mudado de True para False
) -> List[Dict[str, Any]]:
    """
    VersÃ£o sÃ­ncrona da funÃ§Ã£o de busca de ofertas.

    Args:
        max_paginas: NÃºmero mÃ¡ximo de pÃ¡ginas para buscar
        min_desconto: Percentual mÃ­nimo de desconto
        categoria: Categoria de produtos
        min_preco: PreÃ§o mÃ­nimo
        max_preco: PreÃ§o mÃ¡ximo
        apenas_frete_gratis: Se True, retorna apenas ofertas com frete grÃ¡tis
        apenas_lojas_oficiais: Se True, retorna apenas ofertas de lojas oficiais

    Returns:
        Lista de dicionÃ¡rios contendo as ofertas encontradas
    """
    try:
        # Executa a funÃ§Ã£o assÃ­ncrona em um novo loop de eventos
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            return loop.run_until_complete(
                buscar_ofertas_promobit(
                    max_paginas=max_paginas,
                    min_desconto=min_desconto,
                    categoria=categoria,
                    min_preco=min_preco,
                    max_preco=max_preco,
                    apenas_frete_gratis=apenas_frete_gratis,
                    apenas_lojas_oficiais=apenas_lojas_oficiais,
                )
            )
        finally:
            loop.close()

    except Exception as e:
        logger.error(f"Erro na versÃ£o sÃ­ncrona: {e}")
        return []


# ===== FUNÃ‡ÃƒO COMPATIBILIDADE COM SCRAPER REGISTRY =====

async def get_ofertas(periodo: str = "24h") -> List[Dict[str, Any]]:
    """
    FunÃ§Ã£o de compatibilidade com o scraper registry.
    
    Args:
        periodo: PerÃ­odo para coleta (24h, 7d, 30d, all)
        
    Returns:
        Lista de ofertas encontradas
    """
    try:
        # Buscar ofertas de informÃ¡tica (categoria principal)
        ofertas = await buscar_ofertas_promobit(
            max_paginas=2,
            min_desconto=0,
            categoria="informatica"
        )
        
        # Adicionar metadados de compatibilidade
        for oferta in ofertas:
            oferta['fonte'] = 'promobit_scraper'
            oferta['periodo'] = periodo
            oferta['timestamp'] = time.time()
        
        return ofertas
        
    except Exception as e:
        logger.error(f"âŒ Erro na funÃ§Ã£o get_ofertas: {e}")
        return []

# ConfiguraÃ§Ãµes para o scraper registry
priority = 80  # Prioridade alta (site especializado)
rate_limit = 1.0  # 1 requisiÃ§Ã£o por segundo
description = "Scraper para o Promobit - Site especializado em ofertas de informÃ¡tica"

if __name__ == "__main__":
    print("ğŸ” Testando scraper do Promobit...")
    print("=" * 50)

    try:
        ofertas = buscar_ofertas_promobit_sync(max_paginas=1, min_desconto=0)

        if ofertas:
            print(f"âœ… Encontradas {len(ofertas)} ofertas!")
            print("\nğŸ“‹ Primeiras 3 ofertas encontradas:")
            print("-" * 50)

            for i, oferta in enumerate(ofertas[:3], 1):
                print(f"\n{i}. {oferta['titulo'][:60]}...")
                print(f"   ğŸ’° PreÃ§o: {oferta['preco']}")
                if oferta["preco_original"]:
                    print(f"   ğŸ’¸ PreÃ§o Original: {oferta['preco_original']}")
                print(f"   ğŸª Loja: {oferta['loja']}")
                print(f"   ğŸ”— URL: {oferta['url_produto'][:80]}...")
                print(f"   ğŸ†” ID: {oferta['id_produto']}")
                if oferta["desconto"] > 0:
                    print(f"   ğŸ“‰ Desconto: {oferta['desconto']:.1f}%")
        else:
            print("âŒ Nenhuma oferta encontrada")

    except Exception as e:
        print(f"âŒ Erro durante o teste: {e}")
        import traceback

        traceback.print_exc()
