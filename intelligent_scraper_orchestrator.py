#!/usr/bin/env python3
"""
Orquestrador Inteligente de Scrapers
Coordena todos os scrapers usando o sistema anti-duplicidade
"""
import asyncio
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# Sistema Anti-Duplicidade
from duplicate_prevention_system import DuplicatePreventionSystem

# Scrapers
from promobit_scraper_fixed import buscar_ofertas_promobit
from pelando_scraper_fixed import buscar_ofertas_pelando
from zoom_scraper import ZoomScraper
from meupc_scraper import buscar_ofertas_meupc
from buscape_scraper_fixed import buscar_ofertas_buscape
from magalu_scraper import MagazineLuizaScraper

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('intelligent_orchestrator.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('intelligent_orchestrator')

@dataclass
class ScraperConfig:
    """Configura√ß√£o de um scraper"""
    name: str
    domain: str
    function_name: str
    enabled: bool = True
    priority: float = 1.0
    max_retries: int = 3
    retry_delay: int = 300  # segundos

@dataclass
class ScrapingResult:
    """Resultado de uma execu√ß√£o de scraper"""
    scraper_name: str
    domain: str
    success: bool
    products_found: int
    unique_products: int
    execution_time: float
    error_message: Optional[str] = None
    timestamp: Optional[datetime] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

class IntelligentScraperOrchestrator:
    """Orquestrador inteligente de scrapers"""
    
    def __init__(self):
        self.dps = DuplicatePreventionSystem()
        self.scrapers: Dict[str, ScraperConfig] = {}
        self.results_history: List[ScrapingResult] = []
        self.running = False
        
        # Inicializa configura√ß√µes dos scrapers
        self.initialize_scrapers()
        
        # Estat√≠sticas
        self.stats = {
            "total_executions": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "total_products_found": 0,
            "total_unique_products": 0,
            "start_time": datetime.now(),
            "last_execution": datetime.now()
        }
    
    def initialize_scrapers(self):
        """Inicializa configura√ß√µes dos scrapers"""
        self.scrapers = {
            "promobit": ScraperConfig(
                name="Promobit",
                domain="promobit.com.br",
                function_name="promobit",
                priority=1.5,  # Alta prioridade (ofertas mudam r√°pido)
                max_retries=3
            ),
            "pelando": ScraperConfig(
                name="Pelando",
                domain="pelando.com.br",
                function_name="pelando",
                priority=1.5,  # Alta prioridade
                max_retries=3
            ),
            "meupc": ScraperConfig(
                name="MeuPC.net",
                domain="meupc.net",
                function_name="meupc",
                priority=1.1,  # M√©dia prioridade
                max_retries=2
            ),
            "buscape": ScraperConfig(
                name="Buscap√©",
                domain="buscape.com.br",
                function_name="buscape",
                priority=1.0,  # Prioridade padr√£o
                max_retries=2
            ),
            "magalu": ScraperConfig(
                name="Magazine Luiza",
                domain="magazineluiza.com.br",
                function_name="magalu",
                priority=0.9,  # Prioridade menor
                max_retries=2
            )
        }
    
    async def start(self):
        """Inicia o orquestrador"""
        logger.info("üöÄ Iniciando Orquestrador Inteligente de Scrapers")
        self.running = True
        self.stats["start_time"] = datetime.now()
        
        # Executa um ciclo inicial for√ßado
        logger.info("üöÄ Executando ciclo inicial for√ßado...")
        await self.run_cycle_forced()
        
        try:
            while self.running:
                await self.run_cycle()
                await asyncio.sleep(60)  # Aguarda 1 minuto entre ciclos
                
        except KeyboardInterrupt:
            logger.info("üëã Encerrando orquestrador...")
            self.running = False
        except Exception as e:
            logger.error(f"‚ùå Erro fatal no orquestrador: {e}")
            self.running = False
    
    async def run_cycle(self):
        """Executa um ciclo de scraping"""
        logger.info("üîÑ Iniciando ciclo de scraping...")
        
        # Obt√©m fila de processamento
        queue = self.dps.get_processing_queue()
        
        if not queue:
            logger.info("‚è∞ Nenhum site dispon√≠vel para processamento")
            # Aguarda um pouco e tenta novamente
            await asyncio.sleep(30)
            return
        
        # Filtra scrapers habilitados e dispon√≠veis
        available_scrapers = []
        for scraper_id, scraper_config in self.scrapers.items():
            if not scraper_config.enabled:
                continue
            
            # Verifica se o dom√≠nio do scraper est√° na fila de processamento
            if scraper_config.domain in [item["domain"] for item in queue]:
                available_scrapers.append((scraper_id, scraper_config))
                logger.info(f"‚úÖ {scraper_config.name} dispon√≠vel para execu√ß√£o")
            else:
                logger.debug(f"‚è≥ {scraper_config.name} n√£o dispon√≠vel ainda")
        
        if not available_scrapers:
            logger.info("‚ö†Ô∏è Nenhum scraper dispon√≠vel para execu√ß√£o")
            # Aguarda um pouco e tenta novamente
            await asyncio.sleep(30)
            return
        
        # Ordena por prioridade
        available_scrapers.sort(key=lambda x: x[1].priority, reverse=True)
        
        logger.info(f"üéØ Executando {len(available_scrapers)} scrapers dispon√≠veis")
        
        # Executa scrapers em paralelo (m√°ximo 3 simult√¢neos)
        semaphore = asyncio.Semaphore(3)
        
        tasks = []
        for scraper_id, scraper_config in available_scrapers:
            task = asyncio.create_task(
                self.execute_scraper_with_semaphore(semaphore, scraper_id, scraper_config)
            )
            tasks.append(task)
        
        # Aguarda conclus√£o de todos
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Processa resultados
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå Erro na execu√ß√£o: {result}")
            elif isinstance(result, ScrapingResult):
                self.process_result(result)
            else:
                logger.warning(f"‚ö†Ô∏è Resultado inesperado: {type(result)}")
        
        # Atualiza estat√≠sticas
        self.update_stats()
        
        # Salva cache
        self.dps.save_cache()
        
        logger.info("‚úÖ Ciclo de scraping conclu√≠do")
    
    async def run_cycle_forced(self):
        """Executa um ciclo de scraping for√ßado (ignora intervalos de tempo)"""
        logger.info("üöÄ Executando ciclo inicial for√ßado...")
        
        # Executa todos os scrapers habilitados
        available_scrapers = []
        for scraper_id, scraper_config in self.scrapers.items():
            if scraper_config.enabled:
                available_scrapers.append((scraper_id, scraper_config))
                logger.info(f"‚úÖ {scraper_config.name} inclu√≠do no ciclo inicial")
        
        if not available_scrapers:
            logger.warning("‚ö†Ô∏è Nenhum scraper habilitado para execu√ß√£o")
            return
        
        # Executa scrapers em paralelo (m√°ximo 3 simult√¢neos)
        semaphore = asyncio.Semaphore(3)
        
        tasks = []
        for scraper_id, scraper_config in available_scrapers:
            task = asyncio.create_task(
                self.execute_scraper_with_semaphore(semaphore, scraper_id, scraper_config)
            )
            tasks.append(task)
        
        # Aguarda conclus√£o de todos
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Processa resultados
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"‚ùå Erro na execu√ß√£o: {result}")
            elif isinstance(result, ScrapingResult):
                self.process_result(result)
            else:
                logger.warning(f"‚ö†Ô∏è Resultado inesperado: {type(result)}")
        
        # Atualiza estat√≠sticas
        self.update_stats()
        
        # Salva cache
        self.dps.save_cache()
        
        logger.info("‚úÖ Ciclo inicial for√ßado conclu√≠do")
    
    async def execute_scraper_with_semaphore(self, semaphore: asyncio.Semaphore, 
                                           scraper_id: str, scraper_config: ScraperConfig):
        """Executa um scraper com controle de concorr√™ncia"""
        async with semaphore:
            return await self.execute_scraper(scraper_id, scraper_config)
    
    async def execute_scraper(self, scraper_id: str, scraper_config: ScraperConfig) -> ScrapingResult:
        """Executa um scraper espec√≠fico"""
        start_time = time.time()
        logger.info(f"üîç Executando {scraper_config.name} ({scraper_config.domain})")
        
        try:
            # Executa scraper
            products = await self.run_scraper_function(scraper_config.function_name)
            
            if products:
                # Filtra produtos duplicados
                unique_products = self.dps.filter_duplicate_products(products, scraper_config.domain)
                
                # Marca site como processado
                self.dps.mark_site_processed(scraper_config.domain, len(unique_products))
                
                execution_time = time.time() - start_time
                
                result = ScrapingResult(
                    scraper_name=scraper_config.name,
                    domain=scraper_config.domain,
                    success=True,
                    products_found=len(products),
                    unique_products=len(unique_products),
                    execution_time=execution_time,
                    timestamp=datetime.now()
                )
                
                logger.info(f"‚úÖ {scraper_config.name}: {len(unique_products)} produtos √∫nicos "
                           f"de {len(products)} encontrados em {execution_time:.2f}s")
                
                return result
            else:
                # Nenhum produto encontrado
                self.dps.mark_site_processed(scraper_config.domain, 0)
                
                execution_time = time.time() - start_time
                
                result = ScrapingResult(
                    scraper_name=scraper_config.name,
                    domain=scraper_config.domain,
                    success=True,
                    products_found=0,
                    unique_products=0,
                    execution_time=execution_time,
                    timestamp=datetime.now()
                )
                
                logger.info(f"‚ö†Ô∏è {scraper_config.name}: Nenhum produto encontrado em {execution_time:.2f}s")
                
                return result
                
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            logger.error(f"‚ùå {scraper_config.name}: Erro - {error_msg}")
            
            # Marca como processado mesmo com erro para evitar loop infinito
            self.dps.mark_site_processed(scraper_config.domain, 0)
            
            result = ScrapingResult(
                scraper_name=scraper_config.name,
                domain=scraper_config.domain,
                success=False,
                products_found=0,
                unique_products=0,
                execution_time=execution_time,
                error_message=error_msg,
                timestamp=datetime.now()
            )
            
            return result
    
    async def run_scraper_function(self, function_name: str) -> List[Dict[str, Any]]:
        """Executa fun√ß√£o de scraper espec√≠fica"""
        try:
            if function_name == "promobit":
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    return await buscar_ofertas_promobit(session)
            elif function_name == "pelando":
                # Usa o scraper com Selenium para o Pelando
                from pelando_scraper_selenium_fixed import buscar_ofertas_pelando_selenium
                return await buscar_ofertas_pelando_selenium()
            elif function_name == "meupc":
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    return await buscar_ofertas_meupc(session)
            elif function_name == "buscape":
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    return await buscar_ofertas_buscape(session)
            elif function_name == "magalu":
                scraper = MagazineLuizaScraper(headless=True)
                return await asyncio.to_thread(scraper.buscar_ofertas, max_paginas=2)
            else:
                logger.warning(f"‚ö†Ô∏è Fun√ß√£o de scraper '{function_name}' n√£o implementada")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao executar {function_name}: {e}")
            return []
    
    def process_result(self, result: ScrapingResult):
        """Processa resultado de um scraper"""
        self.results_history.append(result)
        
        # Mant√©m apenas os √∫ltimos 1000 resultados
        if len(self.results_history) > 1000:
            self.results_history = self.results_history[-1000:]
        
        # Atualiza estat√≠sticas
        self.stats["total_executions"] += 1
        
        if result.success:
            self.stats["successful_executions"] += 1
        else:
            self.stats["failed_executions"] += 1
        
        self.stats["total_products_found"] += result.products_found
        self.stats["total_unique_products"] += result.unique_products
        self.stats["last_execution"] = result.timestamp
    
    def update_stats(self):
        """Atualiza estat√≠sticas gerais"""
        if self.stats["start_time"]:
            uptime = datetime.now() - self.stats["start_time"]
            self.stats["uptime"] = str(uptime).split('.')[0]  # Remove microssegundos
    
    def get_detailed_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas detalhadas"""
        stats = self.stats.copy()
        
        # Adiciona estat√≠sticas do sistema anti-duplicidade
        dps_stats = self.dps.get_site_statistics()
        stats["duplicate_prevention"] = dps_stats
        
        # Estat√≠sticas por scraper
        scraper_stats = {}
        for scraper_id, scraper_config in self.scrapers.items():
            scraper_results = [r for r in self.results_history if r.domain == scraper_config.domain]
            
            if scraper_results:
                scraper_stats[scraper_id] = {
                    "total_executions": len(scraper_results),
                    "successful_executions": len([r for r in scraper_results if r.success]),
                    "failed_executions": len([r for r in scraper_results if not r.success]),
                    "total_products_found": sum(r.products_found for r in scraper_results),
                    "total_unique_products": sum(r.unique_products for r in scraper_results),
                    "average_execution_time": sum(r.execution_time for r in scraper_results) / len(scraper_results),
                    "last_execution": max((r.timestamp for r in scraper_results if r.timestamp is not None), default=None) if scraper_results else None
                }
        
        stats["scrapers"] = scraper_stats
        
        return stats
    
    def print_status(self):
        """Imprime status atual do orquestrador"""
        print("\n" + "="*60)
        print("üéØ STATUS DO ORQUESTRADOR INTELIGENTE")
        print("="*60)
        
        # Status geral
        print(f"üîÑ Status: {'üü¢ Executando' if self.running else 'üî¥ Parado'}")
        if self.stats["start_time"]:
            print(f"‚è∞ Iniciado em: {self.stats['start_time'].strftime('%d/%m/%Y %H:%M:%S')}")
            if "uptime" in self.stats:
                print(f"‚è±Ô∏è  Uptime: {self.stats['uptime']}")
        
        print(f"üìä Total de execu√ß√µes: {self.stats['total_executions']}")
        print(f"‚úÖ Sucessos: {self.stats['successful_executions']}")
        print(f"‚ùå Falhas: {self.stats['failed_executions']}")
        print(f"üì¶ Produtos encontrados: {self.stats['total_products_found']}")
        print(f"üíé Produtos √∫nicos: {self.stats['total_unique_products']}")
        
        # Fila de processamento
        print(f"\nüîÑ Fila de Processamento:")
        queue = self.dps.get_processing_queue()
        for item in queue:
            print(f"   {item['name']} ({item['domain']}) - Prioridade: {item['priority']:.2f}")
        
        # Status dos scrapers
        print(f"\nüîß Status dos Scrapers:")
        for scraper_id, scraper_config in self.scrapers.items():
            status = "üü¢" if scraper_config.enabled else "üî¥"
            print(f"   {status} {scraper_config.name}: {'Habilitado' if scraper_config.enabled else 'Desabilitado'}")
        
        print("="*60)

async def main():
    """Fun√ß√£o principal"""
    print("üöÄ INICIANDO ORQUESTRADOR INTELIGENTE DE SCRAPERS")
    print("=" * 60)
    
    # Cria orquestrador
    orchestrator = IntelligentScraperOrchestrator()
    
    # Mostra status inicial
    orchestrator.print_status()
    
    try:
        # Inicia orquestrador
        await orchestrator.start()
        
    except KeyboardInterrupt:
        print("\nüëã Encerrando orquestrador...")
        orchestrator.running = False
        
        # Mostra estat√≠sticas finais
        print("\nüìä Estat√≠sticas Finais:")
        final_stats = orchestrator.get_detailed_stats()
        for key, value in final_stats.items():
            if isinstance(value, dict):
                print(f"   {key}:")
                for sub_key, sub_value in value.items():
                    print(f"     {sub_key}: {sub_value}")
            else:
                print(f"   {key}: {value}")
    
    print("\n‚úÖ Orquestrador encerrado com sucesso!")

if __name__ == "__main__":
    asyncio.run(main())
