"""
Stub para scraper da Americanas.
Para habilitar, configure as vari√°veis de ambiente necess√°rias.

‚ö†Ô∏è AVISO: Scraping da Americanas pode ser bloqueado rapidamente.
Recomenda-se usar proxy e rota√ß√£o de User-Agents.

Vari√°veis de ambiente:
- PROXY_URL: URL do proxy (altamente recomendado)
- PROXY_USER: Usu√°rio do proxy (se necess√°rio)
- PROXY_PASS: Senha do proxy (se necess√°rio)
- GG_ALLOW_SCRAPING: Deve ser "1" para habilitar

Para implementar:
1. Implementar fun√ß√£o get_ofertas(periodo: str)
2. Adicionar rate limiting adequado (m√°ximo 1 req/seg)
3. Implementar rota√ß√£o de User-Agents
4. Adicionar retry com backoff exponencial
5. Testar com proxy para evitar bloqueios
"""

import os
import asyncio
import logging
from typing import List, Dict, Any


# Configura√ß√µes
name = "americanas_scraper"
priority = 80  # Prioridade baixa devido ao risco de bloqueio
rate_limit = 0.5  # 0.5 requests por segundo (muito conservador)
retry_count = 5  # Mais tentativas devido ao risco
retry_delay = 3.0  # Delay maior entre tentativas

# URLs base (para refer√™ncia futura)
BASE_URL = "https://www.americanas.com.br"
DOMAIN = "www.americanas.com.br"

# P√°ginas de ofertas (para implementa√ß√£o futura)
OFFER_PAGES = [
    "/ofertas",
    "/ofertas/mais-vendidos",
    "/ofertas/lancamentos",
    "/ofertas/black-friday",
    "/ofertas/cyber-monday"
]

# Categorias populares (para implementa√ß√£o futura)
CATEGORIES = [
    "/eletronicos",
    "/informatica",
    "/games",
    "/casa",
    "/moda",
    "/beleza"
]


def enabled() -> bool:
    """Verifica se o scraper est√° habilitado."""
    # Verificar se scraping √© permitido
    if os.getenv("GG_SEED") and os.getenv("GG_FREEZE_TIME"):
        return False  # Modo CI
    
    # Verificar se h√° proxy configurado (recomendado)
    has_proxy = bool(os.getenv("PROXY_URL"))
    
    if not has_proxy:
        logging.getLogger(f"scraper.{name}").warning(
            "‚ö†Ô∏è Americanas scraper sem proxy - risco alto de bloqueio"
        )
    
    return os.getenv("GG_ALLOW_SCRAPING") == "1"


async def get_ofertas(periodo: str) -> List[Dict[str, Any]]:
    """
    Stub para coleta de ofertas da Americanas.
    
    Args:
        periodo: Per√≠odo para coleta (24h, 7d, 30d, all)
        
    Returns:
        Lista vazia (stub n√£o implementado)
    """
    logger = logging.getLogger(f"scraper.{name}")
    
    if not enabled():
        logger.info("‚ö†Ô∏è Scraper desabilitado (modo CI ou GG_ALLOW_SCRAPING != 1)")
        return []
    
    logger.info("‚ö†Ô∏è Stub da Americanas - implementa√ß√£o n√£o dispon√≠vel")
    logger.info("üìù Para implementar:")
    logger.info("  1. Adicionar depend√™ncias: aiohttp, beautifulsoup4")
    logger.info("  2. Implementar scraping de p√°ginas de ofertas")
    logger.info("  3. Adicionar rate limiting rigoroso")
    logger.info("  4. Implementar rota√ß√£o de User-Agents")
    logger.info("  5. Adicionar suporte a proxy")
    
    # Retornar lista vazia (stub)
    return []


# Vari√°veis de ambiente necess√°rias
REQUIRED_ENV_VARS = [
    "PROXY_URL"  # Altamente recomendado
]

# Configura√ß√µes de risco
RISK_LEVEL = "ALTO"
BLOCK_PROBABILITY = "ALTA"
RECOMMENDED_PROXY = True
RATE_LIMIT_STRICT = True


# Teste local (se executado diretamente)
if __name__ == "__main__":
    async def test():
        print("üß™ Testando stub da Americanas...")
        
        if not enabled():
            print("‚ö†Ô∏è Scraper desabilitado")
            return
        
        print("üìã Informa√ß√µes do stub:")
        print(f"  - Nome: {name}")
        print(f"  - Prioridade: {priority}")
        print(f"  - Rate Limit: {rate_limit} req/seg")
        print(f"  - N√≠vel de Risco: {RISK_LEVEL}")
        print(f"  - Proxy Recomendado: {RECOMMENDED_PROXY}")
        print(f"  - Rate Limit Rigoroso: {RATE_LIMIT_STRICT}")
        
        ofertas = await get_ofertas("7d")
        print(f"‚úÖ Stub retornou {len(ofertas)} ofertas")
    
    asyncio.run(test())
