#!/usr/bin/env python3
"""
Scraper da Shopee usando Playwright - Vers√£o 5.0 com An√°lise Din√¢mica da Estrutura
Analisa a estrutura real da p√°gina e adapta os seletores dinamicamente
"""

import asyncio
import logging
import time
import re
from base_playwright_scraper import BasePlaywrightScraper

# Configura√ß√£o de logging
logger = logging.getLogger(__name__)


class ShopeePlaywrightScraperV5(BasePlaywrightScraper):
    """Scraper da Shopee usando Playwright - Vers√£o 5.0 com An√°lise Din√¢mica"""

    def __init__(self, headless: bool = False):
        super().__init__(
            base_url="https://shopee.com.br",
            store_name="Shopee Brasil",
            headless=headless,
        )

        # Categorias populares para buscar ofertas
        self.categorias = [
            "smartphone",
            "notebook",
            "fone de ouvido",
            "smart tv",
            "console de videogame",
            "c√¢mera digital",
            "tablet",
            "smartwatch",
        ]

        # Seletores base para tentar
        self.base_selectors = [
            'a[href*="/product/"]',
            'a[href*="/item/"]',
            '[class*="product"]',
            '[class*="item"]',
            '[class*="card"]',
            '[class*="grid"]',
            '[class*="list"]',
        ]

    async def analyze_page_structure(self, page_content: str):
        """Analisa a estrutura da p√°gina para encontrar padr√µes de produtos"""
        try:
            logger.info("üîç Analisando estrutura da p√°gina...")

            # Procura por padr√µes comuns de produtos
            patterns = {
                "product_links": r'href="([^"]*/(?:product|item)/[^"]*)"',
                "price_patterns": r"R\$\s*(\d+[,\d]*)",
                "title_patterns": r'<[^>]*class="[^"]*(?:title|name)[^"]*"[^>]*>([^<]+)</[^>]*>',
                "image_patterns": r'src="([^"]*\.(?:jpg|jpeg|png|webp))"',
                "discount_patterns": r"(\d+)%\s*(?:off|desconto|redu√ß√£o)",
            }

            findings = {}
            for pattern_name, pattern in patterns.items():
                matches = re.findall(pattern, page_content, re.IGNORECASE)
                findings[pattern_name] = len(matches)
                if matches:
                    logger.info(f"‚úÖ {pattern_name}: {len(matches)} encontrados")
                    if pattern_name == "product_links":
                        logger.info(f"   Exemplos: {matches[:3]}")
                else:
                    logger.warning(f"‚ö†Ô∏è {pattern_name}: 0 encontrados")

            return findings

        except Exception as e:
            logger.error(f"‚ùå Erro ao analisar estrutura: {e}")
            return {}

    async def find_product_containers(self):
        """Encontra containers de produtos usando m√∫ltiplas estrat√©gias"""
        try:
            containers = []

            # Estrat√©gia 1: Procura por links de produto
            product_links = await self.page.query_selector_all(
                'a[href*="/product/"], a[href*="/item/"]'
            )
            if product_links:
                logger.info(f"‚úÖ Encontrados {len(product_links)} links de produto")
                containers.extend(product_links)

            # Estrat√©gia 2: Procura por elementos com classes suspeitas
            class_selectors = [
                '[class*="product"]',
                '[class*="item"]',
                '[class*="card"]',
                '[class*="grid"]',
                '[class*="list"]',
            ]

            for selector in class_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        logger.info(
                            f"‚úÖ Selector {selector}: {len(elements)} elementos"
                        )
                        # Filtra elementos que n√£o s√£o links de produto
                        for elem in elements:
                            if elem not in containers:
                                containers.append(elem)
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erro com selector {selector}: {e}")
                    continue

            # Estrat√©gia 3: Procura por elementos que contenham texto de produto
            try:
                # Procura por elementos que contenham padr√µes de pre√ßo
                all_elements = await self.page.query_selector_all("*")
                for elem in all_elements[:100]:  # Limita a 100 para performance
                    try:
                        text = await elem.text_content()
                        if text and (
                            "R$" in text
                            or "smartphone" in text.lower()
                            or "iphone" in text.lower()
                        ):
                            if elem not in containers:
                                containers.append(elem)
                    except:
                        continue
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è Erro na busca por texto: {e}")

            logger.info(f"üéØ Total de containers encontrados: {len(containers)}")
            return containers

        except Exception as e:
            logger.error(f"‚ùå Erro ao encontrar containers: {e}")
            return []

    async def extract_product_from_container(self, container, categoria: str):
        """Extrai informa√ß√µes de produto de um container"""
        try:
            product_info = {
                "titulo": None,
                "preco": None,
                "link": None,
                "imagem": None,
                "desconto": None,
                "loja": "Shopee Brasil",
                "categoria": categoria,
                "timestamp": time.time(),
            }

            # Tenta extrair t√≠tulo
            title_candidates = []

            # Procura por elementos de t√≠tulo dentro do container
            title_selectors = [
                '[class*="title"]',
                '[class*="name"]',
                "h1",
                "h2",
                "h3",
                "h4",
                "h5",
                "h6",
                "span",
                "div",
                "p",
            ]

            for selector in title_selectors:
                try:
                    title_elem = await container.query_selector(selector)
                    if title_elem:
                        text = await title_elem.text_content()
                        if text and text.strip() and len(text.strip()) > 10:
                            title_candidates.append(text.strip())
                except:
                    continue

            # Se n√£o encontrou t√≠tulo espec√≠fico, tenta do pr√≥prio container
            if not title_candidates:
                try:
                    text = await container.text_content()
                    if text and text.strip():
                        title_candidates.append(text.strip())
                except:
                    pass

            # Seleciona o melhor t√≠tulo
            if title_candidates:
                # Filtra t√≠tulos muito longos ou muito curtos
                filtered_titles = [t for t in title_candidates if 10 < len(t) < 200]
                if filtered_titles:
                    product_info["titulo"] = filtered_titles[0][:100]
                else:
                    product_info["titulo"] = title_candidates[0][:100]

            # Tenta extrair pre√ßo
            try:
                text = await container.text_content()
                if text and "R$" in text:
                    price_match = re.search(r"R\$\s*(\d+[,\d]*)", text)
                    if price_match:
                        product_info["preco"] = price_match.group(1)
                    else:
                        product_info["preco"] = "Pre√ßo n√£o dispon√≠vel"
                else:
                    product_info["preco"] = "Pre√ßo n√£o dispon√≠vel"
            except:
                product_info["preco"] = "Pre√ßo n√£o dispon√≠vel"

            # Tenta extrair link
            try:
                if hasattr(container, "get_attribute"):
                    link = await container.get_attribute("href")
                    if link:
                        if not link.startswith("http"):
                            link = f"{self.base_url}{link}"
                        product_info["link"] = link
                else:
                    link_elem = await container.query_selector("a")
                    if link_elem:
                        link = await link_elem.get_attribute("href")
                        if link:
                            if not link.startswith("http"):
                                link = f"{self.base_url}{link}"
                            product_info["link"] = link
            except:
                pass

            # Tenta extrair imagem
            try:
                img_elem = await container.query_selector("img")
                if img_elem:
                    src = await img_elem.get_attribute("src")
                    if src:
                        product_info["imagem"] = src
            except:
                pass

            # Tenta extrair desconto
            try:
                text = await container.text_content()
                if text:
                    discount_match = re.search(
                        r"(\d+)%\s*(?:off|desconto|redu√ß√£o)", text, re.IGNORECASE
                    )
                    if discount_match:
                        product_info["desconto"] = discount_match.group(1)
            except:
                pass

            # Valida se o produto tem informa√ß√µes m√≠nimas
            if product_info["titulo"] and len(product_info["titulo"]) > 5:
                return product_info
            else:
                return None

        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Erro ao extrair produto: {e}")
            return None

    async def extract_products_dynamic(self, categoria: str):
        """Extrai produtos usando an√°lise din√¢mica da p√°gina"""
        try:
            logger.info(
                f"üîç Extraindo produtos de {categoria} usando an√°lise din√¢mica..."
            )

            # Aguarda carregamento da p√°gina
            await asyncio.sleep(3)

            # Analisa a estrutura da p√°gina
            page_content = await self.page.content()
            structure_analysis = await self.analyze_page_structure(page_content)

            # Encontra containers de produtos
            containers = await self.find_product_containers()

            if not containers:
                logger.warning("‚ö†Ô∏è Nenhum container de produto encontrado")
                return []

            # Extrai produtos dos containers
            products = []
            for i, container in enumerate(containers[:20]):  # Limita a 20 produtos
                try:
                    product = await self.extract_product_from_container(
                        container, categoria
                    )
                    if product:
                        products.append(product)
                        logger.debug(
                            f"‚úÖ Produto {i + 1} extra√≠do: {product['titulo'][:50]}..."
                        )
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è Erro ao extrair produto {i + 1}: {e}")
                    continue

            logger.info(f"üéØ Total de produtos extra√≠dos: {len(products)}")
            return products

        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o din√¢mica: {e}")
            return []

    async def try_category_access_with_analysis(self, categoria: str):
        """Tenta acessar categoria e analisa a estrutura"""
        try:
            logger.info(f"üîç Tentando acesso √† categoria: {categoria}")

            # Mapeia categorias para IDs conhecidos
            category_mapping = {
                "smartphone": "11013230",
                "notebook": "11013231",
                "fone de ouvido": "11013232",
                "smart tv": "11013233",
                "console de videogame": "11013234",
                "c√¢mera digital": "11013235",
                "tablet": "11013236",
                "smartwatch": "11013237",
            }

            category_id = category_mapping.get(categoria, "11013230")
            category_url = f"{self.base_url}/category/{category_id}"

            # Aplica user agent m√≥vel (que funcionou nas vers√µes anteriores)
            mobile_ua = "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1.2 Mobile/15E148 Safari/604.1"
            await self.page.set_extra_http_headers({"User-Agent": mobile_ua})
            await self.page.set_viewport_size({"width": 375, "height": 667})

            # Acessa a categoria
            await self.page.goto(category_url, wait_until="networkidle")
            await asyncio.sleep(5)

            # Verifica se conseguiu acessar
            page_content = await self.page.content()

            if "Entre" in page_content:
                logger.warning("‚ö†Ô∏è P√°gina ainda requer login")
                return []

            if len(page_content) < 10000:
                logger.warning("‚ö†Ô∏è P√°gina muito pequena, poss√≠vel erro")
                return []

            logger.info("‚úÖ Categoria acessada com sucesso")

            # Faz scroll para carregar conte√∫do din√¢mico
            await self.scroll_page_smart(2.0)
            await asyncio.sleep(3)

            # Extrai produtos usando an√°lise din√¢mica
            products = await self.extract_products_dynamic(categoria)

            return products

        except Exception as e:
            logger.error(f"‚ùå Erro no acesso √† categoria: {e}")
            return []

    async def buscar_ofertas_gerais(self):
        """Busca ofertas gerais usando an√°lise din√¢mica"""
        todas_ofertas = []

        if not await self.setup_browser():
            logger.error("‚ùå N√£o foi poss√≠vel configurar o navegador")
            return []

        try:
            logger.info("üöÄ INICIANDO BUSCA DE OFERTAS NA SHOPEE COM PLAYWRIGHT V5.0")
            logger.info("=" * 60)

            # Testa cada categoria
            for categoria in self.categorias:
                try:
                    logger.info(f"\nüîç Buscando: {categoria.upper()}")

                    # Tenta acesso direto √† categoria com an√°lise
                    ofertas_categoria = await self.try_category_access_with_analysis(
                        categoria
                    )

                    if ofertas_categoria:
                        logger.info(
                            f"‚úÖ {categoria}: {len(ofertas_categoria)} ofertas encontradas"
                        )
                        todas_ofertas.extend(ofertas_categoria)
                    else:
                        logger.warning(f"‚ö†Ô∏è {categoria}: Nenhuma oferta encontrada")

                    # Delay entre categorias
                    await asyncio.sleep(self.get_random_delay(3, 6))

                except Exception as e:
                    logger.error(f"‚ùå Erro na categoria {categoria}: {e}")
                    continue

            # Remove duplicatas
            ofertas_unicas = []
            titulos_vistos = set()

            for oferta in todas_ofertas:
                if oferta["titulo"] not in titulos_vistos:
                    ofertas_unicas.append(oferta)
                    titulos_vistos.add(oferta["titulo"])

            logger.info(f"\nüéØ TOTAL DE OFERTAS √öNICAS: {len(ofertas_unicas)}")

        except Exception as e:
            logger.error(f"‚ùå Erro geral na busca: {e}")

        finally:
            await self.close_browser()

        return todas_ofertas


async def main():
    """Fun√ß√£o principal para teste"""
    print("üöÄ TESTANDO SHOPEE SCRAPER COM PLAYWRIGHT V5.0 (AN√ÅLISE DIN√ÇMICA)")
    print("=" * 60)

    scraper = ShopeePlaywrightScraperV5(headless=False)

    # Testa conex√£o primeiro
    if not await scraper.test_connection():
        print("‚ùå N√£o foi poss√≠vel conectar com a Shopee")
        return

    # Busca ofertas
    ofertas = await scraper.buscar_ofertas_gerais()

    print(f"\nüéØ RESULTADO: {len(ofertas)} ofertas encontradas")
    print("=" * 60)

    for i, oferta in enumerate(ofertas[:10], 1):
        print(f"\n{i}. {oferta['titulo']}")
        print(f"   üí∞ Pre√ßo: {oferta['preco']}")
        if oferta.get("desconto"):
            print(f"   üè∑Ô∏è Desconto: {oferta['desconto']}%")
        print(f"   üè™ Loja: {oferta['loja']}")
        print(f"   üìÇ Categoria: {oferta['categoria']}")
        if oferta.get("link"):
            print(f"   üîó Link: {oferta['link'][:80]}...")
        if oferta.get("imagem"):
            print(f"   üñºÔ∏è Imagem: {oferta['imagem'][:50]}...")

    # Salva as ofertas
    if ofertas:
        filename = scraper.save_results(ofertas, "ofertas_shopee_playwright_v5")
        if filename:
            print(f"\nüíæ Ofertas salvas em: {filename}")


if __name__ == "__main__":
    asyncio.run(main())
