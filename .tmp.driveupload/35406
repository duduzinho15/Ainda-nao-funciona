"""
Script orquestrador para busca e publica√ß√£o de ofertas.

Este script coordena a busca de ofertas em diferentes fontes (Magazine Luiza, Promobit, etc.),
verifica duplicatas, gera links de afiliado e publica as ofertas no canal do Telegram.
"""

import asyncio
import logging
import os
import sys
import time
from datetime import datetime
from typing import List, Dict, Any, Tuple

# Adiciona o diret√≥rio raiz ao path para importa√ß√µes
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler("logs/scraper.log")],
)
logger = logging.getLogger(__name__)

# Configura√ß√£o de disponibilidade de m√≥dulos
MAGALU_AVAILABLE = True
PROMOBIT_AVAILABLE = True
PELANDO_AVAILABLE = True
ZOOM_AVAILABLE = True

# Importa√ß√µes adicionais
import aiohttp
from zoom_scraper import ZoomScraper

# Importa√ß√µes dos m√≥dulos personalizados
try:
    import config
    import database

    # Sistema Anti-Duplicidade
    from duplicate_prevention_system import DuplicatePreventionSystem

    # Scrapers
    from magalu_scraper import MagazineLuizaScraper
    from promobit_scraper import buscar_ofertas_promobit
    from pelando_scraper import buscar_ofertas_pelando
    from amazon_scraper import AmazonScraper
    from shopee_api_integration import ShopeeAPIIntegration
    from aliexpress_api import AliExpressAPI
    from mercadolivre_scraper import buscar_ofertas_mercadolivre
    from buscape_scraper import buscar_ofertas_buscape
    from meupc_scraper import buscar_ofertas_meupc
    from affiliate import gerar_link_afiliado
    from telegram_poster import publicar_oferta_automatica

    # Configura√ß√£o de disponibilidade de m√≥dulos
    MAGALU_AVAILABLE = True
    PROMOBIT_AVAILABLE = True
    PELANDO_AVAILABLE = True
except ImportError as e:
    logger.error(f"Erro ao importar m√≥dulos: {e}")
    raise

# Configura√ß√µes
INTERVALO_ENTRE_EXECUCOES = 3600  # 1 hora
MAX_OFERTAS_POR_EXECUCAO = 5  # N√∫mero m√°ximo de ofertas por execu√ß√£o
INTERVALO_ENTRE_PUBLICACOES = 60  # 60 segundos entre publica√ß√µes

# Cria diret√≥rio de logs se n√£o existir
os.makedirs("logs", exist_ok=True)


class ScraperOrchestrator:
    """Classe para orquestrar a busca e publica√ß√£o de ofertas."""

    def __init__(self):
        self.ultima_execucao = datetime.now()
        self.sessao = None
        self.total_ofertas_publicadas = 0
        self.total_ofertas_ignoradas = 0

    async def inicializar(self):
        """Inicializa os recursos necess√°rios."""
        logger.info("üöÄ Inicializando ScraperOrchestrator...")

        # Configura o banco de dados
        try:
            database.setup_database()
            logger.info("‚úÖ Banco de dados configurado")
        except Exception as e:
            logger.error(f"‚ùå Erro ao configurar banco de dados: {e}")
            raise

        logger.info("‚úÖ Inicializa√ß√£o conclu√≠da")

    async def finalizar(self):
        """Libera os recursos utilizados."""
        logger.info("üîí Finalizando ScraperOrchestrator...")
        if self.sessao:
            await self.sessao.close()
            self.sessao = None
        logger.info("‚úÖ Recursos liberados")

    async def buscar_todas_as_ofertas(
        self,
    ) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
        """
        Busca ofertas de todas as fontes configuradas.

        Returns:
            Tuple[List[Dict], Dict]: Lista de ofertas e estat√≠sticas
        """
        logger.info("üîç Iniciando busca por ofertas...")

        # Dicion√°rio para estat√≠sticas
        stats = {"magalu": 0, "promobit": 0, "total": 0, "erros": 0}

        # Lista para armazenar todas as ofertas encontradas
        todas_as_ofertas = []

        try:
            # 2. Busca ofertas do Magazine Luiza (se dispon√≠vel)
            if MAGALU_AVAILABLE:
                logger.info("üîÑ Buscando ofertas no Magazine Luiza...")
                try:
                    scraper = MagazineLuizaScraper(headless=True)
                    ofertas_magalu = await asyncio.to_thread(
                        scraper.buscar_ofertas, max_paginas=2
                    )
                    stats["magalu"] = len(ofertas_magalu)
                    todas_as_ofertas.extend(ofertas_magalu)
                    logger.info(
                        f"‚úÖ Encontradas {len(ofertas_magalu)} ofertas no Magazine Luiza"
                    )
                except Exception as e:
                    logger.error(f"‚ùå Erro ao buscar no Magazine Luiza: {e}")
                    stats["magalu"] = 0

            # 2. Busca ofertas do Promobit (se dispon√≠vel)
            if PROMOBIT_AVAILABLE:
                logger.info("üîÑ Buscando ofertas no Promobit...")
                ofertas_promobit = await buscar_ofertas_promobit(
                    self.sessao,
                    max_paginas=2,  # Limita a 2 p√°ginas para n√£o sobrecarregar
                )
                logger.info(
                    f"‚úÖ Encontradas {len(ofertas_promobit)} ofertas no Promobit"
                )
                todas_as_ofertas.extend(ofertas_promobit)
                await asyncio.sleep(5)  # Delay entre requisi√ß√µes

            # 3. Busca ofertas do Pelando (se dispon√≠vel)
            if PELANDO_AVAILABLE:
                logger.info("üîÑ Buscando ofertas no Pelando...")
                ofertas_pelando = await buscar_ofertas_pelando(
                    self.sessao,
                    max_paginas=1,  # Limita a 1 p√°gina para n√£o sobrecarregar
                )
                logger.info(f"‚úÖ Encontradas {len(ofertas_pelando)} ofertas no Pelando")
                todas_as_ofertas.extend(ofertas_pelando)

            # Filtra ofertas duplicadas (mesma URL de produto)
            ofertas_unicas = {}
            for oferta in todas_as_ofertas:
                url = oferta.get("url_produto")
                if url and url not in ofertas_unicas:
                    ofertas_unicas[url] = oferta

            logger.info(
                f"üìä Total de ofertas √∫nicas encontradas: {len(ofertas_unicas)}"
            )
            return list(ofertas_unicas.values())

        except Exception as e:
            logger.error(f" Erro inesperado ao buscar ofertas: {e}", exc_info=True)
            stats["erros"] += 1
            return [], stats

    async def processar_e_publicar_ofertas(
        self, ofertas: List[Dict[str, Any]]
    ) -> Dict[str, int]:
        """
        Processa as ofertas encontradas e as publica no canal.

        Args:
            ofertas: Lista de ofertas a serem processadas

        Returns:
            Dict: Estat√≠sticas do processamento
        """
        stats = {"total_processadas": 0, "publicadas": 0, "ignoradas": 0, "erros": 0}

        if not ofertas:
            logger.info(" Nenhuma oferta para processar")
            return stats

        logger.info(f" Processando {len(ofertas)} ofertas...")

        # Ordena as ofertas por desconto (maiores descontos primeiro)
        ofertas_ordenadas = sorted(
            ofertas,
            key=lambda x: (
                float(x.get("desconto", "0").replace("%", ""))
                if x.get("desconto") and isinstance(x.get("desconto"), str)
                else 0
            ),
            reverse=True,
        )

        # Limita o n√∫mero de ofertas a serem publicadas
        ofertas_publicar = ofertas_ordenadas[:MAX_OFERTAS_POR_EXECUCAO]

        for oferta in ofertas_publicar:
            stats["total_processadas"] += 1

            try:
                # Verifica se a oferta j√° foi publicada
                if database.oferta_ja_existe(oferta["url_produto"]):
                    logger.debug(
                        f" Oferta j√° publicada: {oferta.get('titulo', 'Sem t√≠tulo')}"
                    )
                    stats["ignoradas"] += 1
                    continue

                # Gera o link de afiliado
                url_afiliado = gerar_link_afiliado(oferta["url_produto"])

                # Prepara os dados da oferta para publica√ß√£o
                dados_publicacao = {
                    "titulo": oferta.get("titulo", "Oferta sem t√≠tulo"),
                    "preco_atual": oferta.get("preco", "Pre√ßo n√£o dispon√≠vel"),
                    "preco_original": oferta.get("preco_original"),
                    "desconto": oferta.get("desconto"),
                    "url_afiliado": url_afiliado,
                    "imagem_url": oferta.get("imagem_url"),
                    "loja": oferta.get("loja", "Desconhecida"),
                    "fonte": oferta.get("fonte", "Desconhecida"),
                }

                # Publica a oferta
                publicada = await publicar_oferta_automatica(dados_publicacao)

                if publicada:
                    # Salva a oferta no banco de dados
                    oferta_db = {
                        "url_produto": oferta["url_produto"],
                        "titulo": oferta.get("titulo", "Oferta sem t√≠tulo"),
                        "preco": oferta.get("preco", "Pre√ßo n√£o dispon√≠vel"),
                        "preco_original": oferta.get("preco_original"),
                        "loja": oferta.get("loja", "Desconhecida"),
                        "fonte": oferta.get("fonte", "Scraper"),
                        "imagem_url": oferta.get("imagem_url"),
                        "asin": oferta.get("asin"),
                    }

                    try:
                        database.adicionar_oferta(oferta_db)
                        logger.info(
                            f" Oferta publicada: {oferta.get('titulo', 'Sem t√≠tulo')}"
                        )
                        stats["publicadas"] += 1

                        # Aguarda um tempo entre publica√ß√µes
                        await asyncio.sleep(INTERVALO_ENTRE_PUBLICACOES)

                    except Exception as db_error:
                        logger.error(
                            f" Erro ao salvar oferta no banco: {db_error}",
                            exc_info=True,
                        )
                        stats["erros"] += 1
                else:
                    logger.error(
                        f" Falha ao publicar oferta: {oferta.get('titulo', 'Sem t√≠tulo')}"
                    )
                    stats["erros"] += 1

            except Exception as e:
                logger.error(f" Erro ao processar oferta: {e}", exc_info=True)
                stats["erros"] += 1
                continue

        return stats

    async def executar_ciclo(self) -> Dict[str, int]:
        """
        Executa um ciclo completo de busca e publica√ß√£o de ofertas.

        Returns:
            Dict: Estat√≠sticas do ciclo
        """
        logger.info("üîÑ Iniciando ciclo de busca e publica√ß√£o de ofertas...")

        stats = {
            "ofertas_encontradas": 0,
            "ofertas_publicadas": 0,
            "ofertas_ignoradas": 0,
            "erros": 0,
            "tempo_execucao": 0,
        }

        start_time = time.time()

        try:
            # Busca ofertas de todas as fontes
            ofertas, busca_stats = await self.buscar_todas_as_ofertas()
            stats["ofertas_encontradas"] = busca_stats.get("total", 0)
            stats["erros"] += busca_stats.get("erros", 0)

            if not ofertas:
                logger.info("‚ÑπÔ∏è Nenhuma oferta encontrada")
                return stats

            # Processa e publica as ofertas
            processamento_stats = await self.processar_e_publicar_ofertas(ofertas)

            # Atualiza estat√≠sticas
            stats["ofertas_publicadas"] = processamento_stats.get("publicadas", 0)
            stats["ofertas_ignoradas"] = processamento_stats.get("ignoradas", 0)
            stats["erros"] += processamento_stats.get("erros", 0)

            logger.info("‚úÖ Ciclo de busca e publica√ß√£o conclu√≠do com sucesso!")

        except Exception as e:
            logger.error(f"‚ùå Erro durante o ciclo de busca: {e}", exc_info=True)
            stats["erros"] += 1
        finally:
            # Atualiza o timestamp da √∫ltima execu√ß√£o e calcula o tempo total
            self.ultima_execucao = datetime.now()
            stats["tempo_execucao"] = int(time.time() - start_time)

            # Log das estat√≠sticas
            logger.info(
                "üìä Estat√≠sticas do ciclo:"
                + f"\n- Ofertas encontradas: {stats['ofertas_encontradas']}"
                + f"\n- Ofertas publicadas: {stats['ofertas_publicadas']}"
                + f"\n- Ofertas ignoradas: {stats['ofertas_ignoradas']}"
                + f"\n- Erros: {stats['erros']}"
                + f"\n- Tempo de execu√ß√£o: {stats['tempo_execucao']} segundos"
            )

        return stats

    async def executar(self):
        """
        Executa o orquestrador em loop cont√≠nuo.

        Este m√©todo fica em loop, executando ciclos de busca e publica√ß√£o
        com um intervalo entre eles.
        """
        logger.info("üöÄ Iniciando orquestrador em modo cont√≠nuo...")

        try:
            while True:
                # Executa um ciclo completo
                stats = await self.executar_ciclo()

                # Calcula o tempo at√© a pr√≥xima execu√ß√£o
                tempo_ate_proxima = max(
                    0, INTERVALO_ENTRE_EXECUCOES - stats["tempo_execucao"]
                )

                if tempo_ate_proxima > 0:
                    minutos = int(tempo_ate_proxima / 60)
                    segundos = int(tempo_ate_proxima % 60)
                    logger.info(
                        f"‚è≥ Pr√≥xima execu√ß√£o em {minutos} minutos e {segundos} segundos..."
                    )
                    await asyncio.sleep(tempo_ate_proxima)

        except asyncio.CancelledError:
            logger.info("üëã Recebido sinal de cancelamento. Encerrando...")
        except Exception as e:
            logger.error(f"‚ùå Erro fatal no orquestrador: {e}", exc_info=True)
            raise


async def main():
    """Fun√ß√£o principal"""
    print("üöÄ SISTEMA DE SCRAPERS UNIFICADO COM ANTI-DUPLICIDADE")
    print("=" * 60)

    # Inicializa sistema anti-duplicidade
    dps = DuplicatePreventionSystem()
    print("‚úÖ Sistema anti-duplicidade inicializado")

    # Mostra fila de processamento
    print("\nüîÑ Fila de Processamento:")
    queue = dps.get_processing_queue()
    for item in queue:
        print(
            f"   {item['name']} ({item['domain']}) - Prioridade: {item['priority']:.2f}"
        )

    # Executa scrapers com controle de duplicidade
    await run_scrapers_with_duplicate_prevention(dps)

    # Mostra estat√≠sticas finais
    print("\nüìä Estat√≠sticas Finais:")
    stats = dps.get_site_statistics()
    for key, value in stats.items():
        if isinstance(value, dict):
            print(f"   {key}:")
            for sub_key, sub_value in value.items():
                print(f"     {sub_key}: {sub_value}")
        else:
            print(f"   {key}: {value}")

    # Salva cache
    dps.save_cache()
    print("\nüíæ Cache salvo com sucesso!")


async def run_scrapers_with_duplicate_prevention(dps: DuplicatePreventionSystem):
    """Executa scrapers com controle de duplicidade"""
    print("\nüîç Iniciando execu√ß√£o dos scrapers...")

    # Lista de scrapers dispon√≠veis com seus dom√≠nios
    scrapers = [
        {
            "name": "Promobit",
            "domain": "promobit.com.br",
            "function": "promobit",
            "enabled": PROMOBIT_AVAILABLE,
        },
        {
            "name": "Zoom",
            "domain": "zoom.com.br",
            "function": "zoom",
            "enabled": ZOOM_AVAILABLE,
        },
        {
            "name": "Pelando",
            "domain": "pelando.com.br",
            "function": "pelando",
            "enabled": PELANDO_AVAILABLE,
        },
        {
            "name": "MeuPC.net",
            "domain": "meupc.net",
            "function": "meupc",
            "enabled": True,
        },
        {
            "name": "Buscap√©",
            "domain": "buscape.com.br",
            "function": "buscape",
            "enabled": True,
        },
    ]

    for scraper in scrapers:
        if not scraper["enabled"]:
            print(f"‚ö†Ô∏è {scraper['name']} desabilitado")
            continue

        domain = scraper["domain"]
        print(f"\nüîç Verificando {scraper['name']} ({domain})...")

        if dps.can_process_site(domain):
            print("   ‚úÖ Pode ser processado")

            try:
                # Executa scraper espec√≠fico
                products = await execute_scraper(scraper["function"])

                if products:
                    # Filtra produtos duplicados
                    unique_products = dps.filter_duplicate_products(products, domain)
                    print(f"   üì¶ Produtos √∫nicos: {len(unique_products)}")

                    # Marca site como processado
                    dps.mark_site_processed(domain, len(unique_products))

                    # Processa produtos √∫nicos
                    await process_unique_products(unique_products, scraper["name"])
                else:
                    print("   ‚ùå Nenhum produto encontrado")
                    dps.mark_site_processed(domain, 0)

            except Exception as e:
                print(f"   ‚ùå Erro: {e}")
                dps.mark_site_processed(domain, 0)
        else:
            next_time = dps.get_next_processing_time(domain)
            print(f"   ‚è∞ Pr√≥ximo processamento: {next_time.strftime('%H:%M:%S')}")


async def execute_scraper(scraper_type: str) -> List[Dict[str, Any]]:
    """Executa um scraper espec√≠fico"""
    try:
        if scraper_type == "promobit":
            return await buscar_ofertas_promobit()
        elif scraper_type == "zoom":
            scraper = ZoomScraper()
            return await scraper.buscar_ofertas_zoom()
        elif scraper_type == "pelando":
            async with aiohttp.ClientSession() as session:
                return await buscar_ofertas_pelando(session)
        elif scraper_type == "meupc":
            async with aiohttp.ClientSession() as session:
                return await buscar_ofertas_meupc(session)
        elif scraper_type == "buscape":
            async with aiohttp.ClientSession() as session:
                return await buscar_ofertas_buscape(session)
        else:
            print(f"   ‚ö†Ô∏è Scraper {scraper_type} n√£o implementado")
            return []
    except Exception as e:
        print(f"   ‚ùå Erro ao executar {scraper_type}: {e}")
        return []


async def process_unique_products(products: List[Dict[str, Any]], scraper_name: str):
    """Processa produtos √∫nicos encontrados"""
    print(f"   üîÑ Processando {len(products)} produtos √∫nicos...")

    # Aqui voc√™ pode adicionar l√≥gica para:
    # - Salvar no banco de dados
    # - Enviar para o Telegram
    # - Gerar links de afiliado
    # - etc.

    for i, product in enumerate(products[:5], 1):  # Mostra apenas os primeiros 5
        print(f"     {i}. {product.get('titulo', 'N/A')[:50]}...")
        print(f"        üí∞ R$ {product.get('preco_atual', 'N/A')}")
        print(f"        üè™ {product.get('loja', 'N/A')}")

    if len(products) > 5:
        print(f"     ... e mais {len(products) - 5} produtos")


if __name__ == "__main__":
    # Configura o n√≠vel de log para DEBUG se executado diretamente
    logger.setLevel(logging.DEBUG)

    # Executa o script e sai com o c√≥digo de status apropriado
    exit_code = asyncio.run(main())
    exit(exit_code)
